{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c273b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\vyast\\anaconda3\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c58bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2bd699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vyast\\anaconda3\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for eriktks/conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eriktks/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"eriktks/conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "491c5d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c1f00",
   "metadata": {},
   "source": [
    "### BertTokenizerFast is a class from the Hugging Face transformers library designed for tokenizing text inputs using the BERT model's vocabulary. The \"Fast\" version uses a Rust implementation for tokenization, which is faster than the standard Python implementation.        Usage: This tokenizer converts text into tokens that correspond to the vocabulary of a BERT model, and then into input IDs, attention masks, and other necessary inputs for the model. This is essential for preparing raw text data for BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a5aec",
   "metadata": {},
   "source": [
    "### DataCollatorForTokenClassification is a utility class provided by the transformers library to dynamically pad the inputs for token classification tasks to the length of the longest sequence in a batch. This is useful when working with models that expect inputs of equal length.                                                                      Usage: In token classification tasks like NER, different sentences have different lengths. This data collator pads all sentences in a batch to the length of the longest sentence in that batch, making it easier to handle variable-length inputs during training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499f67e",
   "metadata": {},
   "source": [
    "### AutoModelForTokenClassification is a generic class from the transformers library designed to automatically instantiate a model suitable for token classification tasks, such as NER. It will automatically select the appropriate architecture (e.g., BERT, RoBERTa) based on the provided model identifier. Usage: This class simplifies the process of loading pre-trained models for token classification tasks. You can load a model by specifying the model name or path, and it will return a model ready to be fine-tuned or used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c934357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from transformers import BertTokenizerFast \n",
    "from transformers import DataCollatorForTokenClassification \n",
    "from transformers import AutoModelForTokenClassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08dcb5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69b2cfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de84448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51091486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1242d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")  #bert-base-uncased\" model. The \"uncased\" variant means the tokenizer will convert all text to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edd7a60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38216257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' As we can see, it returns a list with the same number of elements as our processed input ids, mapping special tokens to None and all other tokens to their respective word. This way, we can align the labels with the processed input ids. '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = data['train'][0]\n",
    "\n",
    "tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words=True) #s_split_into_words=True: This indicates that the input is already split into words (a list of words).\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "print(word_ids)  #start of sen and end of sen\n",
    "\n",
    "''' As we can see, it returns a list with the same number of elements as our processed input ids, mapping special tokens to None and all other tokens to their respective word. This way, we can align the labels with the processed input ids. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6f211b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'eu',\n",
       " 'rejects',\n",
       " 'german',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'british',\n",
       " 'lamb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens   ## cls>sos  sep>eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbb20c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 11)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text['ner_tags']), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4236164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True): \n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) \n",
    "    labels = [] \n",
    "    for i, label in enumerate(examples[\"ner_tags\"]): \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) \n",
    "\n",
    "        previous_word_idx = None \n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids: \n",
    "            if word_idx is None: \n",
    "               \n",
    "                label_ids.append(-100)\n",
    "           \n",
    "            elif word_idx != previous_word_idx:\n",
    "                                 \n",
    "                label_ids.append(label[word_idx]) \n",
    "            else: \n",
    "                \n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100) \n",
    "                 \n",
    "            previous_word_idx = word_idx \n",
    "        labels.append(label_ids) \n",
    "    tokenized_inputs[\"labels\"] = labels \n",
    "    return tokenized_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82bf27d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['4'],\n",
       " 'tokens': [['Germany',\n",
       "   \"'s\",\n",
       "   'representative',\n",
       "   'to',\n",
       "   'the',\n",
       "   'European',\n",
       "   'Union',\n",
       "   \"'s\",\n",
       "   'veterinary',\n",
       "   'committee',\n",
       "   'Werner',\n",
       "   'Zwingmann',\n",
       "   'said',\n",
       "   'on',\n",
       "   'Wednesday',\n",
       "   'consumers',\n",
       "   'should',\n",
       "   'buy',\n",
       "   'sheepmeat',\n",
       "   'from',\n",
       "   'countries',\n",
       "   'other',\n",
       "   'than',\n",
       "   'Britain',\n",
       "   'until',\n",
       "   'the',\n",
       "   'scientific',\n",
       "   'advice',\n",
       "   'was',\n",
       "   'clearer',\n",
       "   '.']],\n",
       " 'pos_tags': [[22,\n",
       "   27,\n",
       "   21,\n",
       "   35,\n",
       "   12,\n",
       "   22,\n",
       "   22,\n",
       "   27,\n",
       "   16,\n",
       "   21,\n",
       "   22,\n",
       "   22,\n",
       "   38,\n",
       "   15,\n",
       "   22,\n",
       "   24,\n",
       "   20,\n",
       "   37,\n",
       "   21,\n",
       "   15,\n",
       "   24,\n",
       "   16,\n",
       "   15,\n",
       "   22,\n",
       "   15,\n",
       "   12,\n",
       "   16,\n",
       "   21,\n",
       "   38,\n",
       "   17,\n",
       "   7]],\n",
       " 'chunk_tags': [[11,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   11,\n",
       "   12,\n",
       "   12,\n",
       "   11,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   21,\n",
       "   13,\n",
       "   11,\n",
       "   12,\n",
       "   21,\n",
       "   22,\n",
       "   11,\n",
       "   13,\n",
       "   11,\n",
       "   1,\n",
       "   13,\n",
       "   11,\n",
       "   17,\n",
       "   11,\n",
       "   12,\n",
       "   12,\n",
       "   21,\n",
       "   1,\n",
       "   0]],\n",
       " 'ner_tags': [[5,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   3,\n",
       "   4,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   5,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fafb622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_and_align_labels(data['train'][4:5]) \n",
    "print(q) \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21adc4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "germany_________________________________ 5\n",
      "'_______________________________________ 0\n",
      "s_______________________________________ 0\n",
      "representative__________________________ 0\n",
      "to______________________________________ 0\n",
      "the_____________________________________ 0\n",
      "european________________________________ 3\n",
      "union___________________________________ 4\n",
      "'_______________________________________ 0\n",
      "s_______________________________________ 0\n",
      "veterinary______________________________ 0\n",
      "committee_______________________________ 0\n",
      "werner__________________________________ 1\n",
      "z_______________________________________ 2\n",
      "##wing__________________________________ 2\n",
      "##mann__________________________________ 2\n",
      "said____________________________________ 0\n",
      "on______________________________________ 0\n",
      "wednesday_______________________________ 0\n",
      "consumers_______________________________ 0\n",
      "should__________________________________ 0\n",
      "buy_____________________________________ 0\n",
      "sheep___________________________________ 0\n",
      "##me____________________________________ 0\n",
      "##at____________________________________ 0\n",
      "from____________________________________ 0\n",
      "countries_______________________________ 0\n",
      "other___________________________________ 0\n",
      "than____________________________________ 0\n",
      "britain_________________________________ 5\n",
      "until___________________________________ 0\n",
      "the_____________________________________ 0\n",
      "scientific______________________________ 0\n",
      "advice__________________________________ 0\n",
      "was_____________________________________ 0\n",
      "clearer_________________________________ 0\n",
      "._______________________________________ 0\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]): \n",
    "    print(f\"{token:_<40} {label}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b19d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying on entire data\n",
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e581aa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'input_ids': [101,\n",
       "  7327,\n",
       "  19164,\n",
       "  2446,\n",
       "  2655,\n",
       "  2000,\n",
       "  17757,\n",
       "  2329,\n",
       "  12559,\n",
       "  1012,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "113af597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Defining model\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32f47ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vyast\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Define training args\n",
    "from transformers import TrainingArguments, Trainer \n",
    "\n",
    "\n",
    "args = TrainingArguments( \n",
    "\"test-ner\",\n",
    "evaluation_strategy = \"epoch\", \n",
    "learning_rate=2e-5, \n",
    "per_device_train_batch_size=16, \n",
    "per_device_eval_batch_size=16, \n",
    "num_train_epochs=3, \n",
    "weight_decay=0.01, \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5e678bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf2cc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in c:\\users\\vyast\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from seqeval) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from seqeval) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vyast\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1122e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vyast\\AppData\\Local\\Temp\\ipykernel_20108\\2437978075.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"seqeval\")\n",
      "C:\\Users\\vyast\\anaconda3\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets \n",
    "metric = datasets.load_metric(\"seqeval\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2298e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8a04bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = data[\"train\"].features[\"ner_tags\"].feature.names \n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06ae5100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "7\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in example[\"ner_tags\"]:\n",
    "    print(i)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdb09364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[\"ner_tags\"]] \n",
    "labels\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34c8748f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[labels], references=[labels]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4627693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds): \n",
    "    pred_logits, labels = eval_preds \n",
    "    \n",
    "    pred_logits = np.argmax(pred_logits, axis=2) \n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we donâ€™t need to apply the softmax\n",
    "    \n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [ \n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "        for prediction, label in zip(pred_logits, labels) \n",
    "    ] \n",
    "    \n",
    "    true_labels = [ \n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "       for prediction, label in zip(pred_logits, labels) \n",
    "   ] \n",
    "    results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "    return { \n",
    "          \"precision\": results[\"overall_precision\"], \n",
    "          \"recall\": results[\"overall_recall\"], \n",
    "          \"f1\": results[\"overall_f1\"], \n",
    "          \"accuracy\": results[\"overall_accuracy\"], \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca803354",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer( \n",
    "   model, \n",
    "   args, \n",
    "   train_dataset=tokenized_datasets[\"train\"], \n",
    "   eval_dataset=tokenized_datasets[\"validation\"], \n",
    "   data_collator=data_collator, \n",
    "   tokenizer=tokenizer, \n",
    "   compute_metrics=compute_metrics \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea0ef651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2634/2634 30:31:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>0.065420</td>\n",
       "      <td>0.914790</td>\n",
       "      <td>0.927173</td>\n",
       "      <td>0.920940</td>\n",
       "      <td>0.981842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.057579</td>\n",
       "      <td>0.927419</td>\n",
       "      <td>0.939143</td>\n",
       "      <td>0.933244</td>\n",
       "      <td>0.984368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.059253</td>\n",
       "      <td>0.927343</td>\n",
       "      <td>0.940933</td>\n",
       "      <td>0.934089</td>\n",
       "      <td>0.984892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2634, training_loss=0.07869316134021964, metrics={'train_runtime': 109897.1915, 'train_samples_per_second': 0.383, 'train_steps_per_second': 0.024, 'total_flos': 1020143109346326.0, 'train_loss': 0.07869316134021964, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f91e98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6b7bf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9b1295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21f58688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'O',\n",
       " '1': 'B-PER',\n",
       " '2': 'I-PER',\n",
       " '3': 'B-ORG',\n",
       " '4': 'I-ORG',\n",
       " '5': 'B-LOC',\n",
       " '6': 'I-LOC',\n",
       " '7': 'B-MISC',\n",
       " '8': 'I-MISC'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "804fc93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': '0',\n",
       " 'B-PER': '1',\n",
       " 'I-PER': '2',\n",
       " 'B-ORG': '3',\n",
       " 'I-ORG': '4',\n",
       " 'B-LOC': '5',\n",
       " 'I-LOC': '6',\n",
       " 'B-MISC': '7',\n",
       " 'I-MISC': '8'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5145edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10cf7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40459609",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24a879b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad5f5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9954983, 'index': 1, 'word': 'raju', 'start': 0, 'end': 4}, {'entity': 'B-MISC', 'score': 0.47512144, 'index': 5, 'word': 'apple', 'start': 18, 'end': 23}, {'entity': 'B-ORG', 'score': 0.97722775, 'index': 7, 'word': 'apple', 'start': 27, 'end': 32}, {'entity': 'B-LOC', 'score': 0.9983321, 'index': 12, 'word': 'india', 'start': 52, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "example = \"raju is eating an apple at apple office which is in india\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc50334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec170b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebd88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
